"""
NL2SQL ç³»ç»Ÿè¯„ä¼°å·¥å…·
ä½¿ç”¨ NL2SQL train.json æ•°æ®é›†è¯„ä¼°ç³»ç»Ÿæ€§èƒ½
"""

import json
import sys
import os
from typing import List, Dict, Tuple
from collections import defaultdict

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))


class NL2SQLEvaluator:
    """NL2SQL è¯„ä¼°å™¨"""
    
    def __init__(self, dataset_path: str):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            dataset_path: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        """
        self.dataset_path = dataset_path
        self.data = []
        self.load_dataset()
    
    def load_dataset(self):
        """åŠ è½½æ•°æ®é›†"""
        print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {self.dataset_path}")
        
        with open(self.dataset_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(self.data)} æ¡æ•°æ®\n")
    
    def analyze_dataset(self):
        """åˆ†ææ•°æ®é›†ç»“æ„"""
        print("=" * 80)
        print("ğŸ“Š æ•°æ®é›†åˆ†æ")
        print("=" * 80)
        
        # åŸºæœ¬ç»Ÿè®¡
        print(f"\n1ï¸âƒ£  åŸºæœ¬ä¿¡æ¯:")
        print(f"   æ€»æ ·æœ¬æ•°: {len(self.data)}")
        print(f"   æ ·æœ¬å­—æ®µ: {list(self.data[0].keys())}")
        
        # æ•°æ®åº“ç»Ÿè®¡
        db_counts = defaultdict(int)
        for item in self.data:
            db_counts[item['db_id']] += 1
        
        print(f"\n2ï¸âƒ£  æ•°æ®åº“åˆ†å¸ƒ:")
        print(f"   å”¯ä¸€æ•°æ®åº“æ•°: {len(db_counts)}")
        print(f"   å‰5ä¸ªæ•°æ®åº“:")
        for i, (db_id, count) in enumerate(sorted(db_counts.items(), key=lambda x: -x[1])[:5], 1):
            print(f"     {i}. {db_id}: {count} æ¡æŸ¥è¯¢")
        
        # SQL ç±»å‹ç»Ÿè®¡
        agg_types = defaultdict(int)
        cond_conn_types = defaultdict(int)
        
        for item in self.data:
            sql = item['sql']
            # èšåˆç±»å‹
            for agg in sql.get('agg', []):
                agg_types[agg] += 1
            # æ¡ä»¶è¿æ¥ç±»å‹
            cond_conn_types[sql.get('cond_conn_op', 0)] += 1
        
        print(f"\n3ï¸âƒ£  SQL ç‰¹å¾ç»Ÿè®¡:")
        print(f"   èšåˆå‡½æ•°åˆ†å¸ƒ:")
        agg_names = {0: 'NONE', 1: 'MAX', 2: 'MIN', 3: 'COUNT', 4: 'COUNT', 5: 'SUM', 6: 'AVG'}
        for agg, count in sorted(agg_types.items()):
            print(f"     {agg_names.get(agg, f'AGG_{agg}')}: {count}")
        
        print(f"\n   æ¡ä»¶è¿æ¥ç¬¦åˆ†å¸ƒ:")
        conn_names = {0: 'NONE', 1: 'AND', 2: 'OR'}
        for conn, count in sorted(cond_conn_types.items()):
            print(f"     {conn_names.get(conn, f'CONN_{conn}')}: {count}")
        
        # é—®é¢˜é•¿åº¦ç»Ÿè®¡
        question_lengths = [len(item['question']) for item in self.data]
        avg_length = sum(question_lengths) / len(question_lengths)
        
        print(f"\n4ï¸âƒ£  é—®é¢˜ç‰¹å¾:")
        print(f"   å¹³å‡é—®é¢˜é•¿åº¦: {avg_length:.1f} å­—ç¬¦")
        print(f"   æœ€çŸ­é—®é¢˜: {min(question_lengths)} å­—ç¬¦")
        print(f"   æœ€é•¿é—®é¢˜: {max(question_lengths)} å­—ç¬¦")
        
        # æ ·æœ¬ç¤ºä¾‹
        print(f"\n5ï¸âƒ£  æ•°æ®æ ·æœ¬ç¤ºä¾‹:")
        for i in range(min(3, len(self.data))):
            item = self.data[i]
            print(f"\n   ç¤ºä¾‹ {i+1}:")
            print(f"     æ•°æ®åº“ID: {item['db_id']}")
            print(f"     é—®é¢˜: {item['question']}")
            print(f"     SQL: {item['query']}")
            print(f"     é—®é¢˜ID: {item['question_id']}")
    
    def get_evaluation_subset(self, n: int = 100) -> List[Dict]:
        """
        è·å–è¯„ä¼°å­é›†
        
        Args:
            n: å­é›†å¤§å°
            
        Returns:
            è¯„ä¼°æ ·æœ¬åˆ—è¡¨
        """
        import random
        
        # ç¡®ä¿å¯é‡ç°
        random.seed(42)
        
        # æŒ‰æ•°æ®åº“åˆ†å±‚é‡‡æ ·
        db_samples = defaultdict(list)
        for item in self.data:
            db_samples[item['db_id']].append(item)
        
        # ä»æ¯ä¸ªæ•°æ®åº“é‡‡æ ·
        subset = []
        samples_per_db = max(1, n // len(db_samples))
        
        for db_id, samples in db_samples.items():
            subset.extend(random.sample(samples, min(samples_per_db, len(samples))))
        
        # å¦‚æœä¸å¤Ÿ,å†éšæœºè¡¥å……
        if len(subset) < n:
            remaining = [item for item in self.data if item not in subset]
            subset.extend(random.sample(remaining, min(n - len(subset), len(remaining))))
        
        return subset[:n]
    
    def evaluate_prediction(
        self, 
        ground_truth: str, 
        prediction: str
    ) -> Tuple[bool, Dict]:
        """
        è¯„ä¼°å•ä¸ªé¢„æµ‹ç»“æœ
        
        Args:
            ground_truth: çœŸå®SQL
            prediction: é¢„æµ‹SQL
            
        Returns:
            (æ˜¯å¦å®Œå…¨åŒ¹é…, è¯„ä¼°è¯¦æƒ…)
        """
        # æ ‡å‡†åŒ–SQL (å»é™¤ç©ºæ ¼ã€å¤§å°å†™ç­‰)
        def normalize_sql(sql: str) -> str:
            sql = sql.strip().upper()
            sql = ' '.join(sql.split())
            return sql
        
        gt_norm = normalize_sql(ground_truth)
        pred_norm = normalize_sql(prediction)
        
        # å®Œå…¨åŒ¹é…
        exact_match = (gt_norm == pred_norm)
        
        # éƒ¨åˆ†åŒ¹é…è¯„ä¼°
        details = {
            'exact_match': exact_match,
            'gt_length': len(ground_truth),
            'pred_length': len(prediction),
            'has_select': 'SELECT' in pred_norm,
            'has_where': 'WHERE' in pred_norm,
            'has_and': 'AND' in pred_norm,
            'has_or': 'OR' in pred_norm,
        }
        
        return exact_match, details
    
    def run_evaluation(
        self,
        executor,
        subset_size: int = 10,
        verbose: bool = True
    ) -> Dict:
        """
        è¿è¡Œè¯„ä¼°
        
        Args:
            executor: NL2SQLæ‰§è¡Œå™¨
            subset_size: è¯„ä¼°æ ·æœ¬æ•°
            verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        print("\n" + "=" * 80)
        print(f"ğŸ¯ å¼€å§‹è¯„ä¼° (æ ·æœ¬æ•°: {subset_size})")
        print("=" * 80)
        
        subset = self.get_evaluation_subset(subset_size)
        
        results = {
            'total': len(subset),
            'exact_match': 0,
            'partial_match': 0,
            'failed': 0,
            'details': []
        }
        
        for i, item in enumerate(subset, 1):
            if verbose:
                print(f"\n[{i}/{len(subset)}] è¯„ä¼°ä¸­...")
                print(f"  é—®é¢˜: {item['question'][:50]}...")
                print(f"  çœŸå®SQL: {item['query'][:60]}...")
            
            try:
                # è¿™é‡Œéœ€è¦å®é™…è°ƒç”¨æ‰§è¡Œå™¨
                # result = executor.execute(item['db_id'], item['question'])
                # predicted_sql = result.data.get('sql', '')
                
                # æ¨¡æ‹Ÿé¢„æµ‹ (å®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºçœŸå®é¢„æµ‹)
                predicted_sql = item['query']  # ä¸´æ—¶ä½¿ç”¨çœŸå®SQLæ¨¡æ‹Ÿ
                
                exact_match, details = self.evaluate_prediction(
                    item['query'],
                    predicted_sql
                )
                
                if exact_match:
                    results['exact_match'] += 1
                    if verbose:
                        print(f"  âœ… å®Œå…¨åŒ¹é…")
                else:
                    results['partial_match'] += 1
                    if verbose:
                        print(f"  âš ï¸  ä¸åŒ¹é…")
                
                results['details'].append({
                    'question_id': item['question_id'],
                    'question': item['question'],
                    'ground_truth': item['query'],
                    'prediction': predicted_sql,
                    'exact_match': exact_match,
                    **details
                })
                
            except Exception as e:
                results['failed'] += 1
                if verbose:
                    print(f"  âŒ å¤±è´¥: {str(e)}")
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = results['exact_match'] / results['total'] * 100
        
        print("\n" + "=" * 80)
        print("ğŸ“ˆ è¯„ä¼°ç»“æœ")
        print("=" * 80)
        print(f"æ€»æ ·æœ¬æ•°: {results['total']}")
        print(f"å®Œå…¨åŒ¹é…: {results['exact_match']} ({accuracy:.2f}%)")
        print(f"éƒ¨åˆ†åŒ¹é…: {results['partial_match']}")
        print(f"æ‰§è¡Œå¤±è´¥: {results['failed']}")
        
        return results
    
    def save_results(self, results: Dict, output_path: str):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("NL2SQL æ•°æ®é›†åˆ†æä¸è¯„ä¼°å·¥å…·")
    print("=" * 80)
    
    # 1. åŠ è½½å’Œåˆ†ææ•°æ®é›†
    dataset_path = "data/NL2SQL/train.json"
    evaluator = NL2SQLEvaluator(dataset_path)
    
    # 2. åˆ†ææ•°æ®é›†
    evaluator.analyze_dataset()
    
    # 3. è¯„ä¼°ç¤ºä¾‹ (éœ€è¦å®é™…çš„æ‰§è¡Œå™¨)
    print("\n" + "=" * 80)
    print("ğŸ’¡ å¦‚ä½•ä½¿ç”¨æ­¤æ•°æ®é›†è¯„ä¼°ç³»ç»Ÿ:")
    print("=" * 80)
    print("""
1. å‡†å¤‡æ‰§è¡Œå™¨:
   ```python
   from src.executors import NL2SQLExecutor
   executor = NL2SQLExecutor(
       schema_file="data/CSpider/db_schema.json",
       llm_config=config.get_llm_config()
   )
   ```

2. è¿è¡Œè¯„ä¼°:
   ```python
   evaluator = NL2SQLEvaluator("data/NL2SQL/train.json")
   results = evaluator.run_evaluation(
       executor=executor,
       subset_size=100,  # è¯„ä¼°100ä¸ªæ ·æœ¬
       verbose=True
   )
   ```

3. æŸ¥çœ‹ç»“æœ:
   ```python
   print(f"å‡†ç¡®ç‡: {results['exact_match'] / results['total'] * 100:.2f}%")
   evaluator.save_results(results, "evaluation_results.json")
   ```

4. å…³é”®è¯„ä¼°æŒ‡æ ‡:
   - Exact Match (EM): ç”Ÿæˆçš„SQLä¸æ ‡å‡†ç­”æ¡ˆå®Œå…¨ä¸€è‡´
   - Execution Accuracy: SQLæ‰§è¡Œç»“æœæ­£ç¡®
   - Component Match: SQLå„ç»„ä»¶(SELECT, WHEREç­‰)æ­£ç¡®ç‡
   
5. æ•°æ®é›†ç‰¹ç‚¹:
   - ä¸­æ–‡NL2SQLæ•°æ®é›†
   - åŒ…å«çœŸå®ä¸šåŠ¡åœºæ™¯æŸ¥è¯¢
   - æä¾›ç»“æ„åŒ–SQLè¡¨ç¤ºå’ŒæŸ¥è¯¢å­—ç¬¦ä¸²
   - é€‚åˆè¯„ä¼°ä¸­æ–‡è¯­ä¹‰ç†è§£å’ŒSQLç”Ÿæˆèƒ½åŠ›
    """)


if __name__ == "__main__":
    main()
